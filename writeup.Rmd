---
title: "Boston Marathon Finish Time Predictions"
subtitle: "Harvard Stats E139 Fall 2015"
author: 
- Nathaniel Burbank
- Pooja Singh
- David Wihl
date: "December 21, 2015"
abstract: |
    We want to build a reasonably accurate model for predicting Boston Marathon finish times based on gender, age on 5k split times. We will build a baseline using untransformed linear regression. Then, we will analyze the data and perform appropriate transformations. After transforming the data, we will cluster the data into different subgroups using unsupervised learning algorithms. Finally, we will run different regression algorithms on the transformed and subsetted data to see our improvements over the baseline.
output: pdf_document
---

```{r echo=FALSE}

# Boston Marathon Finish Time Predictions
# Harvard Stats E139 Fall 2015
# Nathaniel Burbank, Pooja Singh, David Wihl
#
# December 21, 2015


# Common Functions


percent <- function(x, digits = 1, format = "f", ...) {
  paste0(formatC(100 * x, format = format, digits = digits, ...), "%")
}

create_folds = function (df, k){
  # Perform k-Fold Cross Validation and return average score
  # Inspired by Scikit-Learn's cross_val_score
  if (missing(k)) {
    k = 10
  }
  
  train1_start = train1_end = train2_start = train2_end = test_start = test_end = c()
  testsize = nrow(df) / k 
  for (i in 1:k){
    if (i < k){
      train1_start[i] = 1
      train1_end[i] = floor((k-i) * testsize)
    } else {
      train1_start[i] = 0
      train1_end[i] = 0
    }
    test_start[i] = max(ceiling((k-i) * testsize),1)
    test_end[i]   = floor((k-i+1) * testsize)
    train2_start[i] = 0
    train2_end[i] = 0
    if (i > 1){
      train2_start[i] = ceiling((k-i+1) * testsize)
      train2_end[i] = nrow(df)
    }
  }
  return (data.frame(train1_start, train1_end, train2_start, train2_end, test_start, test_end))
}

# we'll do 10-fold cross validation
k = 10 

# Read in the data for multiple years
dfm <- read.csv("Previous Boston Marathon study/BAA data.txt",header=T,sep=" ")
dfm$Age2014 = NULL # remove unneeded column which is mostly NA
ok = complete.cases(dfm)
dfm = dfm[ok,] # remove rows that have any NA values
times = as.matrix(dfm[,7:15], ncol=9)
dfm$totaltime = rowSums(times)
dfm<- dfm[c("totaltime","Age","Gender1F2M","K0.5")] # keep only columns we need
dfm$Gender1F2M = as.factor(dfm$Gender1F2M) # make gender into a factor
dfm = dfm[!is.na(dfm$totaltime), ]  # eliminate rows with no finish times
dfm = dfm[sample(nrow(dfm)),]  # in case the data is sorted, randomize the order
```

## Objectives and Motivation

Boston Marathon Runners are very concerned about their finish times, moreso than many other races. Boston has a limited course size,
constrainted by the 30' width of road in Hopkington, MA ([source](http://www.coolrunning.com/engine/6/6_1/baa-marathon-faq.shtml)).
Due to the limitation, Boston has to be very selective in the choice of runners and has put restrictive qualifying times in order
to run the marathon ([source](http://www.baa.org/Races/Boston-Marathon/Participant-Information/Qualifying.aspx)). In order to run
the Boston Marathon, not only does an applicant had to have run a previous qualifying marathon, the candidate must also had to finish
much faster than the typical marathoner. While other races like New York and Chicago have increased their participation size significantly
in recent years, Boston is currently and the foreseeable future restricted to approximately 27,000 runners.

```{r echo=FALSE}
# Find mean finish time by gender
agg = aggregate(dfm$totaltime, by=list(dfm$Gender1F2M), FUN=mean)[2]
men = as.integer(agg$x[2])
women = as.integer(agg$x[1])
```



Mean Finish Time(min)    Men         Women
--------------------    ---------    -------
Worldwide(*)            253           282   
Boston(**)              `r men`             `r women`


(*) [The Guardian](http://www.theguardian.com/lifeandstyle/the-running-blog/2015/apr/21/marathons-by-numbers-running-the-data)

(**) See Data Sources, below

Boston is also known as a hilly course so a candidate who may have run a fast race on a flat course would be challenged by New England's
rolling hills, especally between miles 16-21, which is among the most difficult of the course. The peak of course is affectionately known
as Heartbreak Hill.

For all these reasons plus the long history of the race, the Boston Marathon is a very prestigious event in the running community. 
Forecasting an accurate finish time in the race is a key motivator for runners to improve their time during the hundreds of miles of training
a marathon requires.

## Data Sources

We obtained our data from two sources. First, we downloaded a [previously assembled dataset](http://www.stat.unc.edu/faculty/rs/Bostonwebpage/readme.html)  from a group of researchers University of North Carolina-Chapel Hill who [analyzed](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0093800) historic runner performance with the goal of predicting the anticpated finish times for the approximately 5000 runners who were unable to complete the 2013 Boston Marathon due the bombings at the finish line. This data set contained that split and finish times for `r nrow(dfm)` runners in the 2010, 2011, and 2013 Boston Marathons. (The 2012 Boston marathon was intentionally excluded from their dataset because it was unusually hot that year.) We also obtained finish and split times for 78042 runners who completed the Chicago Marathon in 2014 and 2015. We obtained these results directly from the Chicago Marathonâ€™s [results website](http://results.chicagomarathon.com/2015/)  using a custom python scraper. 

## Methodology

We are attempting to build the most accurate prediction model possible on the smallest amount of interesting data. We are not attempting to 
infer or analyze the data extensively. Our method consists of the following steps:

1. Data Cleansing
1. Create a baseline regression using OLS
1. Analyze the data and perform appropriate transformations
1. Cluster the data into subgroups using unsupervised learning, such as $k$-means
1. Verify the improved accuracy of our predictions.

Throughout we will be using 10-fold cross validation testing with sum of squares errors as the metric to improve.

## Data Cleansing

Even though we downloaded the data from another study that had performed some scrubbing, we needed to perform additional data cleansing.
We had to eliminate any rows that were missing any split times as imputation might have tainted the predictive model. Since we were using 
$k$-Fold cross-validation, we randomized the order of the data to ensure that each fold had a reasonable sample. Finally, gender had a
value of {1, 2} which we had to convert into a factor in R.

### Removing Outliers

```{r echo=FALSE}
# Remove outliers (5k > 3 sigma from mean)
hist (dfm$K0.5, breaks=15, main="Distribution of 5k Split Times", xlab="5k Split Time (min)")
mean5k = mean(dfm$K0.5)
sd5k = sd(dfm$K0.5)
outliers5k = dfm$K0.5>(mean5k + 3*sd5k)
outliers5ksum = sum(outliers5k)
dfm.rows = nrow(dfm)
dfm = dfm[!outliers5k,]
malepct = table(dfm$Gender1F2M)[2]  / (table(dfm$Gender1F2M)[2] + table(dfm$Gender1F2M)[1])
```

As the histogram reveals, there are several outliers in 5k split times. These could be due to timing errors,
an injury or accident at the start of the race. Due to OLS sensitivity, we elected to remove outliers
that were more than three standard deviations greater than the mean. In total there were `r outliers5ksum` rows like this out of 
`r dfm.rows` (`r percent(outliers5ksum / as.numeric(dfm.rows))`).

Age was in the range [18,81]. (The minimum age to run the Boston Marathon is 18). No outliers were removed. Males represented
`r percent(malepct)` of total runners, which seemed reasonable.

## Baseline Regression
```{r echo=FALSE, results="hide"}
# Create a set of k-fold sets for cross-validation
folds = create_folds(dfm,k)
score = c()

# Baseline regression
base.mod = lm(totaltime~.,data=dfm)

# get baseline score 
for (i in 1:k) {
  train = dfm[c(folds[i,"train1_start"]:folds[i,"train1_end"],folds[i,"train2_start"]:folds[i,"train2_end"]),]
  test = dfm[folds[i,"test_start"]:folds[i,"test_end"],]

  model = lm(totaltime~.,data=train)
  score[i] = sum((test$totaltime - predict(model,new=test))^2) / as.numeric(nrow(test))
}
score.mean = mean(score)
```

We ran a simple OLS regression using all the Boston data. The predictor variables were age, gender (as factor), and 5k split time. The response variable
is the finish time in minutes.

The baseline model was quite good, but we felt we could do better.

```{r echo=FALSE}
summary(base.mod)
```

Using $k$-Fold cross validation, and taking the sum of squares error for each fold resulted in a baseline mean score of `r as.integer(score.mean)`
or approximately `r as.integer(sqrt(score.mean))` minutes.
While $\pm$ `r as.integer(sqrt(score.mean))` minutes over the course of a four hour race may not seem like a large error,
it makes a significant difference in terms of ability to qualify for Boston.

## Examination of the Data and Transformations

```{r echo=FALSE}
# Display histograms of response variable untransformed and log transformed
par(mfrow=c(1,2))
hist(dfm$totaltime,breaks=50, main="Untransformed", xlab="Finish time (min)")
hist(log(dfm$totaltime),breaks=50, main="Log Transformed", xlab="Finish time (min)")
```

The untransformed data appears slightly right skewed. A log transformation of the response variable results in a more normal distribution.


```{r echo=FALSE}
#par(mfrow=c(2,2))
#plot(model, pch=23 ,bg="chocolate1",cex=.8)


#y.hat = predict(base.mod)
#xydata = data.frame(x=y.hat, y=resid(base.mod))
#xydata = xydata[sample(1:nrow(xydata), 5000, replace=FALSE),]
#plot(xydata$x,xydata$y,ylim=c(-100,100), xlab="Predicted", ylab="Residuals")
```


```{r echo=FALSE}

#plot(dfms$totaltime,model.resid,ylim=c(-100,100))
#y.hat = predict(tx.mod)
#xydata = data.frame(x=y.hat, y=resid(tx.mod))
#xydata = xydata[sample(1:nrow(xydata), 5000, replace=FALSE),]
#plot(xydata$x,xydata$y,ylim=c(-100,100), xlab="Predicted", ylab="Residuals")
#par(mfrow=c(2,2))
#plot(tx.mod, pch=23 ,bg="chocolate1",cex=.8)
```
Some preliminary EDA of the data:

The data appears somewhat right skewed. Let's try a log transform of the predicted variable:


That seems better.

Let's try a multiple regression on the log transformed data:


Our transformed mean error is 



Let's plot the residuals vs predicted

This appears to be spreading out and trending up. 

Let's try a polynomial transformation on the 5k time to see if that improves things:

## Step 3: Clustering the Data into Subgroups

Let's try subsetting the data set in subgroups using an unsupervised learning algorithm.

```{r echo=FALSE}
# Warning: this takes awhile to run because it includes all data points
# install.packages("fpc")
#library(fpc)
#plotcluster(dfm2, fit.km$cluster)
```

**Note: as you can see, the unsupervised learning found groupings that would have difficult if not
impossible to us to find on our own**

## Conclusion and Next Steps

By transforming and subsetting the data we were able to bring down the size of the residuals significantly and improve the accuracy of the model.

Potential next steps of this project include:

* Making the prediction model available on a public website using R/Shiny. We fill that our prediction model is far more
accurate than what is currently available to runners.


# Appendix A - Code

The following is the code used to prepare this report:
```{r code=readLines(knitr::purl('writeup.Rmd', documentation = 0)), eval = FALSE}

```

# Appendix B - References

This entire project code including source data can be found on GitHub ([https://github.com/wihl/statse139-project](https://github.com/wihl/statse139-project))

[Place R Code in Appendix](http://stackoverflow.com/questions/33485893/create-appendix-with-r-code-in-rmarkdown-knitr)

[Takes mean by subgroup in R](http://stats.stackexchange.com/questions/8225/how-to-summarize-data-by-group-in-r)

[Formatting R Values as Percent](http://stackoverflow.com/questions/7145826/how-to-format-a-number-as-percentage-in-r)